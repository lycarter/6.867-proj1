\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage{float}
\title{[placeholder]6.867 Project 1[placeholder]\vspace{-3.5ex}}
\begin{document}
\newgeometry{top=.25in,left=1in,right=1in,bottom=1in}
\maketitle
\vspace{-4em}
%Graphic = $$\includegraphics[width=WIDTH]{DiffEq_Diagram_#######}$$
\section{}
First, we discuss variations on gradient descent, including analytic gradient descent, finite difference gradient descent, and \texttt{Matlab}'s \texttt{fminunc}, which includes some variations such as adaptive step size, using a quadratic approximation instead of linear approximation, and using a 2-dimensional subspace to reduce computational complexity. First, we'll examine the analytic gradient descent, which relies on having an analytic gradient expression at all points in space.

We'll refer to three example functions: the $n$-dimensional quadratic ``bowl'', $Q_n$, the $n$-dimensional inverted Gaussian (centered at $\mu = 0$, with $\Sigma = \mathbf I_n$), $N_n$, and the $n$-dimensional sum of $\sin$'s, $S_n$. These are defined as follows (leaving off the normalization constant on the inverted Gaussian for simplicity):

\[ Q_n = \left\| \mathbf x \right\| ^2 \]

\[ N_n = -\exp{\left(-\dfrac{1}{2} \left\| \mathbf x \right\| ^2 \right)} \]

\[ S_n = \sum_{i=1}^n \sin(x_n) \]
Next, we benchmark the gradient descent variations on these two functions (seeding with an initial guess of $(1,1,\ldots)$):

\begin{table}[h]
\centering
\caption{Iterations (step = .1, threshold = .001)}
\begin{tabular}{r|ccc|ccc|ccc}
   & \multicolumn{3}{|c}{Analytic Gradient} & \multicolumn{3}{|c}{Finite Differences} & \multicolumn{3}{|c}{\texttt{fminfunc}} \\
$n$& $Q_n$         & $N_n$        & $S_n$   & $Q_n$              & $N_n$   & $S_n$           & $Q_n$         & $N_n$        & $S_n$ \\\hline
2  & 16            & 33           & 46      & 16                 & 33      & 46              &               &              &       \\
5  & 18            & 60           & 50      & 18                 & 60      & 50              &               &              &       \\
20 & 21            &  1           & 57      & 21                 &  1      & 57              &               &              &       \\
100& 25            &  1           & 64      & 25                 &  1      & 64              &               &              &       
\end{tabular}
\end{table}

We notice that the finite differences and analytic methods yield the exact same number of iterations for all functions. This can be explained by examining the diagonal of the Hessian matrix for each of the functions - in all cases, the values are small, indicating low curvature, and thus that the function can be well-approximated by the linear finite difference approximation.

\end{document}













